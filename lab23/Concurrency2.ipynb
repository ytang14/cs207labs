{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUTURES \n",
    "\n",
    "A future, or promise, is something that represents a pending opearion and returns straight away. One can then query their state of completion, or register callbacks to be called on successful completion or error.\n",
    "\n",
    "Examples Adapted from Fluent Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, uuid, functools\n",
    "def get_thing_maker(secs, item):\n",
    "    time.sleep(secs)\n",
    "    return str(uuid.uuid4())+str(item)\n",
    "get_thing = functools.partial(get_thing_maker, 1)\n",
    "def get_many(lot):\n",
    "    counter=0\n",
    "    for t in lot:\n",
    "        thing = get_thing(t)\n",
    "        counter += 1\n",
    "    return counter\n",
    "def serial_main(it):\n",
    "    t0 = time.time()\n",
    "    count = get_many(it)\n",
    "    elapsed = time.time() - t0\n",
    "    msg = '\\n{} things got in {:.2f}s' \n",
    "    print(msg.format(count, elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serial sleeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serial_main(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concurrent sleeping using threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "def get_many_threaded1(it):\n",
    "    workers = 10\n",
    "    with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        res = executor.map(get_thing, it)\n",
    "    return len(list(res))\n",
    "def threaded_main1(it):\n",
    "    t0 = time.time()\n",
    "    count = get_many_threaded1(it)\n",
    "    elapsed = time.time() - t0\n",
    "    msg = '\\n{} things got in {:.2f}s' \n",
    "    print(msg.format(count, elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threaded_main1(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might think that the concurrent IO (or sleeping) case is limited by the GIL, but in both cases, the GIL is yielded. Thus there is no waiting around.\n",
    "\n",
    "The GIL is harmless if code is being run in the context of python library IO or code running in properly coded C extensions like numpy.  The time.sleep() function also releases the GIL. Python threads are totally usable in I/O-bound applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n",
    "\n",
    "threads vs processes\n",
    "\n",
    "On linux\n",
    "\n",
    "- processes created by fork()\n",
    "- have a primary thread\n",
    "- thread is the unit of execution\n",
    "- process is a container, can have more threads\n",
    "- can be scheduled across different cores/cpus\n",
    "\n",
    "```c\n",
    "int pid;\n",
    "int status = 0;\n",
    "/* fork returns pid of child to parent and 0 to child*/\n",
    "if (pid = fork()) {\n",
    "    /* parent code */\n",
    "    pid = wait(&status);\n",
    "    /*wait returns child pid and status*/\n",
    "} else {\n",
    "    /* child  code*/\n",
    "    exit(status);\n",
    "} \n",
    "```\n",
    "\n",
    "- threads in a process share same address space (share it entirely)\n",
    "- thread abstraction decouples resource allocation from control\n",
    "- defines a single sequential execution stream with PC, stack, register values\n",
    "- process handles: address space, global variables, open files, child processes, pending alarms, signals and signal handlers, accounting info\n",
    "- thread handles program counter, registers, stack, and state\n",
    "- user vs kernel threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    return fib(n - 1) + fib(n - 2) if n > 1 else n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "from time import time\n",
    "\n",
    "\n",
    "def sleepy(): #like io\n",
    "    i=0\n",
    "    while i < 10:\n",
    "        print(\"{} -- {} Sleepy!\".format(i, int(time())), flush=True)\n",
    "        sleep(3)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def cpuy():\n",
    "    for i in range(35):\n",
    "        val = fib(i)\n",
    "        print(\"fib({}) is {}\".format(i, val))\n",
    "\n",
    "def cpuy2():\n",
    "    for i in range(35):\n",
    "        val = fib(i)\n",
    "        print(\"cpuy2 fib({}) is {}\".format(i, val))\n",
    "        \n",
    "def main():\n",
    "    # Second thread will print the hello message. Starting as a daemon means\n",
    "    # the thread will not prevent the process from exiting.\n",
    "    start = time()\n",
    "    cpuy()\n",
    "    cpuy2()\n",
    "    print(\"serial elapsed:\", time() - start)\n",
    "    start=time()\n",
    "    #t = Thread(target=sleepy)\n",
    "    #t.start()\n",
    "    t2 = Thread(target=cpuy2)\n",
    "    t2.start()\n",
    "    # Main thread will read and process input\n",
    "    cpuy()\n",
    "    print(\"thread elapsed:\", time() - start)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes with concurrent futures.\n",
    "\n",
    "CPU based processing wont release the gil, and is thus best done in a separate process. For illustration, we show what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def get_many_process(it, workers=None):\n",
    "    if workers:\n",
    "        with futures.ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "            res = executor.map(get_thing, it)\n",
    "    else:\n",
    "        with futures.ProcessPoolExecutor() as executor:\n",
    "            res = executor.map(get_thing, it)\n",
    "    return len(list(res))\n",
    "\n",
    "def process_main(it, workers=None):\n",
    "    t0 = time.time()\n",
    "    count = get_many_process(it, workers)\n",
    "    elapsed = time.time() - t0\n",
    "    msg = '\\n{} things got in {:.2f}s' \n",
    "    print(msg.format(count, elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_main(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_main(range(20), workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "start = time.time()\n",
    "p=multiprocessing.Process(target=cpuy2)\n",
    "p.start()\n",
    "cpuy()\n",
    "p.join()\n",
    "print(\"mp elapsed:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input('>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sockets\n",
    "\n",
    "- distinction between \"client socket\" and \"server socket\"\n",
    "- default `socket.socket(family=AF_INET, type=SOCK_STREAM, proto=0, fileno=None)`\n",
    "- server socket sits and creates client sockets\n",
    "- non-blocking sockets and the `select` system call\n",
    "\n",
    "Read: https://docs.python.org/3.5/howto/sockets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a web page fetcher\n",
    "\n",
    "We'll eventually use the asyncio module to play with web page fetching and crawling, but lets build up to that by writing a simple fetcher. We'll start with blocking, then move to non-blocking, and finally to co-routines, and even more finally to `yield from` based co-routines.\n",
    "\n",
    "Adapted from http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blocking fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "def fetch(host, url):\n",
    "    sock = socket.socket()\n",
    "    sock.connect((host, 80))\n",
    "    request = 'GET {} HTTP/1.0\\r\\nHost: {}\\r\\n\\r\\n'.format(url, host)\n",
    "    print(request)\n",
    "    sock.send(request.encode('ascii'))\n",
    "    response = b''\n",
    "    chunk = sock.recv(4096)\n",
    "    while chunk:\n",
    "        #print(chunk)\n",
    "        response += chunk\n",
    "        chunk = sock.recv(4096)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, IFrame\n",
    "HTML(str(fetch(\"www.example.com\",\"/\")))\n",
    "#bs4.BeautifulSoup(str(fetch(\"www.example.com\",\"/\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic non-blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "host=\"www.example.com\"\n",
    "url=\"/\"\n",
    "request = 'GET {} HTTP/1.0\\r\\nHost: {}\\r\\n\\r\\n'.format(url, host)\n",
    "encoded = request.encode('ascii')\n",
    "sock = socket.socket()\n",
    "sock.setblocking(False)\n",
    "try:\n",
    "    sock.connect(('xkcd.com', 80))\n",
    "except BlockingIOError:\n",
    "    pass\n",
    "while True:\n",
    "    try:\n",
    "        sock.send(encoded)\n",
    "        break  # Done.\n",
    "    except OSError as e:\n",
    "        pass\n",
    "\n",
    "print('sent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has only been implemented partially. Notice how the `sock.send` spins in a loop.\n",
    "\n",
    "This eats cycles. the solution is to use select/kqueue/epoll from a small number of connections to a large number of them. The basic idea behind `select` is to wait for an event to occur on a small set of non-blocking sokets.\n",
    "\n",
    "We'll use python's `DefaultSelector`, an addition from python 3.4 that automatically chooses the \"best\" select like implementation on your system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selectors import DefaultSelector, EVENT_WRITE\n",
    "from time import time\n",
    "selector = DefaultSelector()\n",
    "host=\"www.example.com\"\n",
    "sock = socket.socket()\n",
    "sock.setblocking(False)\n",
    "try:\n",
    "    sock.connect((host, 80))\n",
    "except BlockingIOError:\n",
    "    pass\n",
    "\n",
    "def connected():\n",
    "    selector.unregister(sock.fileno())\n",
    "    print('connected!', flush=True)\n",
    "\n",
    "selector.register(sock.fileno(), EVENT_WRITE, connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`connected` is the **callback** run when the connection happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def loop():\n",
    "    start = time()\n",
    "    while True:\n",
    "        if time() - start > 10:\n",
    "            break\n",
    "        events = selector.select()\n",
    "        for event_key, event_mask in events:\n",
    "            callback = event_key.data\n",
    "            callback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a loop is called an \"event loop\". An async frameworkhas two parts: (a) such an event loop and (b) non-blocking sockets. It all runs on one thread. This is a system, it should be obvious for I/O bound problems.\n",
    "\n",
    "What have we demonstrated already? We showed how to begin an operation and execute a callback when the operation is ready. An async framework builds on the two features we have shown—non-blocking sockets and the event loop—to run concurrent operations on a single thread.\n",
    "\n",
    "Guido:\n",
    ">We have achieved \"concurrency\" here, but not what is traditionally called \"parallelism\". What asynchronous I/O is right for, is applications with many slow or sleepy connections with infrequent events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loop() #loop will destruct after 10 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### async with response reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Implement a URL fetcher using Beautiful Soup in the callback version. We will implement a similar one using coroutines on wednesday. \n",
    "\n",
    "The implimentation will extend the read_response method by parsing for URL's using `bs4` . Start by creating globals:\n",
    "```\n",
    "urls_todo = set(['/'])\n",
    "seen_urls = set(['/'])\n",
    "```\n",
    "\n",
    "then:\n",
    "\n",
    "```\n",
    "links = self.parse_links()#write this\n",
    "```\n",
    "(using self.response)\n",
    "\n",
    "Then use the set `difference` method  to add new links to `urls_todo` and recursively set up a `Fetcher` instance.\n",
    "\n",
    "Now update the `seen_urls` and `urls_todo` thus:\n",
    "```\n",
    "seen_urls.update(links)\n",
    "urls_todo.remove(self.url)\n",
    "if not urls_todo:\n",
    "    stopped = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import urllib\n",
    "import socket\n",
    "urls_todo = set() \n",
    "seen_urls = set()\n",
    "\n",
    "selector = DefaultSelector()\n",
    "stopped = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE\n",
    "\n",
    "selector = DefaultSelector()\n",
    "class Fetcher:\n",
    "    def __init__(self, host, url, level=0):\n",
    "        self.response = b''  # Empty array of bytes.\n",
    "        self.host = host\n",
    "        self.url = url\n",
    "        self.sock = None\n",
    "        \n",
    "    # Method on Fetcher class.\n",
    "    def fetch(self):\n",
    "        self.sock = socket.socket()\n",
    "        self.sock.setblocking(False)\n",
    "        try:\n",
    "            self.sock.connect((self.host, 80))\n",
    "        except BlockingIOError:\n",
    "            pass\n",
    "\n",
    "        # Register next callback.\n",
    "        selector.register(self.sock.fileno(),\n",
    "                          EVENT_WRITE,\n",
    "                          self.connected)\n",
    "\n",
    "    def connected(self, key, mask):\n",
    "        print('connected!', flush=True)\n",
    "        selector.unregister(key.fd)\n",
    "        request = 'GET {} HTTP/1.0\\r\\nHost: {}\\r\\n\\r\\n'.format(self.url, self.host)\n",
    "        self.sock.send(request.encode('ascii'))\n",
    "\n",
    "        # Register the next callback.\n",
    "        selector.register(key.fd,\n",
    "                          EVENT_READ,\n",
    "                          self.read_response)\n",
    "        \n",
    "    def read_response(self, key, mask):\n",
    "        global stopped\n",
    "        \n",
    "        chunk = self.sock.recv(128)  # USUALLY 4k chunk size, here small\n",
    "        if chunk:\n",
    "            print(\"read chunk\", flush=True)\n",
    "            self.response += chunk\n",
    "        else:\n",
    "            print(\"all read\", flush=True)\n",
    "            selector.unregister(key.fd)  # Done reading.\n",
    "            stopped=True\n",
    "            \n",
    "stopped = False\n",
    "\n",
    "def loop():\n",
    "    while not stopped:\n",
    "        events = selector.select()\n",
    "        for event_key, event_mask in events:\n",
    "            callback = event_key.data\n",
    "            callback(event_key, event_mask)\n",
    "        #do fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected!\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "all read\n"
     ]
    }
   ],
   "source": [
    "fetcher = Fetcher('xkcd.com', '/353/',1)\n",
    "fetcher.fetch()\n",
    "urls_todo.add((fetcher.host,fetcher.url))\n",
    "loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the control-flow is chained together by having the connected callback do the resposing. Beyond a 2-3 ladder, this gets confusing and onerous (see some node.js code). As compared to a blocking program, where the continuation of the program is stored and adressed via the instruction pointer in a sequential fashiom, here the cintinuation is stored by registering the callbacks.'\n",
    "\n",
    "Since the current frame is popped out of the stack, exceptions have a hard time figuring the origin This is called stack-ripping.\n",
    "\n",
    "So, even apart from the long debate about the relative efficiencies of multithreading and async, there is this other debate regarding which is more error-prone: threads are susceptible to data races if you make a mistake synchronizing them, but callbacks are stubborn to debug due to stack ripping. And within a bit, we get callback soup.\n",
    "\n",
    "https://thesynchronousblog.wordpress.com/tag/stack-ripping/\n",
    "\n",
    "Threads seem to offer a more natural way of programming as the programmer with all state in thread’s single stack.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why not use them. As we said last time: synchronization and overhead. \n",
    "\n",
    "But we can do better with Coroutines!\n",
    "\n",
    "Guido:\n",
    ">We entice you with a promise. It is possible to write asynchronous code that combines the efficiency of callbacks with the classic good looks of multithreaded programming. This combination is achieved with a pattern called \"coroutines\". Using Python 3.4's standard asyncio library, and a package called \"aiohttp\", fetching a URL in a coroutine is very direct7:\n",
    "\n",
    "    @asyncio.coroutine\n",
    "    def fetch(self, url):\n",
    "        response = yield from self.session.get(url)\n",
    "        body = yield from response.read()\n",
    "        \n",
    "In 3.5 its even more clear:\n",
    "\n",
    "async def fetch(self, url):\n",
    "        response = await self.session.get(url)\n",
    "        body = await response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the Future with co-routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE\n",
    "import socket\n",
    "selector = DefaultSelector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The future, as you might expect is something with callbacks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyFuture:\n",
    "    def __init__(self):\n",
    "        self.result = None\n",
    "        self._callbacks = []\n",
    "\n",
    "    def add_done_callback(self, fn):\n",
    "        self._callbacks.append(fn)\n",
    "\n",
    "    def set_result(self, result):\n",
    "        self.result = result\n",
    "        for fn in self._callbacks:\n",
    "            fn(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a \"main\" to yield to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fetcher:\n",
    "    \n",
    "    def __init__(self, url, host):\n",
    "        self.url = url\n",
    "        self.host = host\n",
    "        self.response = b''  # Empty array of bytes.\n",
    "\n",
    "        \n",
    "    def fetch(self):\n",
    "        global stopped\n",
    "        sock = socket.socket()\n",
    "\n",
    "        sock.setblocking(False)\n",
    "        try:\n",
    "            sock.connect((self.host, 80))\n",
    "        except BlockingIOError:\n",
    "            pass\n",
    "\n",
    "        f = MyFuture()\n",
    "\n",
    "        #resolves the future by setting a result on it\n",
    "        def on_connected():\n",
    "            print('on connected cb ran', flush=True)\n",
    "            f.set_result(None)\n",
    "        \n",
    "        \n",
    "        \n",
    "        selector.register(sock.fileno(),\n",
    "                          EVENT_WRITE,\n",
    "                          on_connected)\n",
    "        print(\"about to yield connection future\", flush=True)\n",
    "        yield f#this makes it look like fetch has returned the \"future\"\n",
    "        #bit we have not lost the state (or have to have carried it in obj)\n",
    "        #a send in will continue us here\n",
    "        print('we were connected! now back in gen', flush=True)\n",
    "        selector.unregister(sock.fileno())\n",
    "        request = 'GET {} HTTP/1.0\\r\\nHost: {}\\r\\n\\r\\n'.format(self.url, self.host)\n",
    "        sock.send(request.encode('ascii'))\n",
    "        while True:\n",
    "            print(\"in loop\")\n",
    "            #now create a new future for the data-recieving call\n",
    "            f = MyFuture()\n",
    "            def on_response():\n",
    "                chunky = sock.recv(4096)  # 4k chunk size.\n",
    "                f.set_result(chunky)\n",
    "            selector.register(sock.fileno(),\n",
    "                              EVENT_READ,\n",
    "                              on_response)\n",
    "            #now to restart the gen, we will from the main\n",
    "            #throw the data right back in\n",
    "            print('=========')\n",
    "            chunk = yield f\n",
    "            \n",
    "            selector.unregister(sock.fileno())\n",
    "            if chunk:\n",
    "                print(\"len(chunk)\",len(chunk))\n",
    "                self.response += chunk\n",
    "            else:\n",
    "                print(\"all read\")\n",
    "                stopped= True\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#But when the future resolves, what resumes the generator? We need a coroutine driver. Let us call it \"task\":\n",
    "#(this is our main)\n",
    "class Task:\n",
    "    def __init__(self, coro):\n",
    "        self.coro = coro\n",
    "        f = MyFuture()\n",
    "        print(\">>sending none to initial future\",f)\n",
    "        f.set_result(None)\n",
    "        print(\"...stepping\")\n",
    "        self.step(f)\n",
    "        print(\">>>after priming\")\n",
    "\n",
    "    def step(self, future):\n",
    "        try:\n",
    "            print(\"sending\", type(future.result))\n",
    "            next_future = self.coro.send(future.result)\n",
    "            print('got next future', next_future)\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"si\")\n",
    "            return None\n",
    "        next_future.add_done_callback(self.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopped=False\n",
    "def loop():\n",
    "    while not stopped:\n",
    "        events = selector.select()\n",
    "        print('got events\\n')\n",
    "        for event_key, event_mask in events:\n",
    "            print('calling back\\n')\n",
    "            callback = event_key.data\n",
    "            print('callback name\\n',callback.__name__)\n",
    "            callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetcher = Fetcher('/353/', 'xkcd.com')\n",
    "Task(fetcher.fetch())\n",
    "stopped=False\n",
    "loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refactoring using generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#But when the future resolves, what resumes the generator? We need a coroutine driver. Let us call it \"task\":\n",
    "#(this is our main)\n",
    "class Task:\n",
    "    def __init__(self, coro):\n",
    "        self.coro = coro\n",
    "        f = MyFuture()\n",
    "        print(\">>sending none to initial future\",f)\n",
    "        f.set_result(None)\n",
    "        print(\"...stepping\")\n",
    "        self.step(f)\n",
    "        print(\">>>after priming\")\n",
    "\n",
    "    def step(self, future):\n",
    "        try:\n",
    "            print(\"sending\", type(future.result))\n",
    "            next_future = self.coro.send(future.result)\n",
    "            print('got next future', next_future)\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"si\")\n",
    "            return None\n",
    "        next_future.add_done_callback(self.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read(sock):\n",
    "    f = MyFuture()\n",
    "\n",
    "    def on_readable():\n",
    "        f.set_result(sock.recv(4096))\n",
    "\n",
    "    selector.register(sock.fileno(), EVENT_READ, on_readable)\n",
    "    chunk = yield f  # Read one chunk.\n",
    "    selector.unregister(sock.fileno())\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_all(sock):\n",
    "    global stopped\n",
    "    response = []\n",
    "    # Read whole response.\n",
    "    chunk = yield from read(sock)\n",
    "    while chunk:\n",
    "        response.append(chunk)\n",
    "        chunk = yield from read(sock)\n",
    "    stopped=True\n",
    "    return b''.join(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If you squint and make the yield from statements disappear it looks like  conventional functions doing blocking I/O. But in fact, read and read_all are coroutines. Yielding from read pauses read_all until the I/O completes. While read_all is paused, asyncio's event loop does other work and awaits other I/O events; read_all is resumed with the result of read on the next loop tick once its event is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fetcher:\n",
    "    \n",
    "    def __init__(self, url, host):\n",
    "        self.url = url\n",
    "        self.host = host\n",
    "        self.response = b''  # Empty array of bytes.\n",
    "\n",
    "        \n",
    "    def fetch(self):\n",
    "        global stopped\n",
    "        sock = socket.socket()\n",
    "\n",
    "        sock.setblocking(False)\n",
    "        try:\n",
    "            sock.connect((host, 80))\n",
    "        except BlockingIOError:\n",
    "            pass\n",
    "\n",
    "        f = MyFuture()\n",
    "\n",
    "        def on_connected():\n",
    "            print('on connected cb ran')\n",
    "            f.set_result(None)\n",
    "        \n",
    "        \n",
    "        \n",
    "        selector.register(sock.fileno(),\n",
    "                          EVENT_WRITE,\n",
    "                          on_connected)\n",
    "        print(\"about to yield connection future\")\n",
    "        yield f\n",
    "        print('connected!')\n",
    "        selector.unregister(sock.fileno())\n",
    "        request = 'GET {} HTTP/1.0\\r\\nHost: xkcd.com\\r\\n\\r\\n'.format(self.url)\n",
    "        sock.send(request.encode('ascii'))\n",
    "        yield from read_all(sock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetcher = Fetcher('/353/', 'xkcd.com')\n",
    "Task(fetcher.fetch())\n",
    "stopped = False\n",
    "loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://aosabook.org/en/500L/crawler-images/yield-from.png)\n",
    "\n",
    "There is one yield left amongst the yield froms. For consistency, this can be fixed...it also lets us change implementations under the hood.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read(sock):\n",
    "    f = MyFuture()\n",
    "\n",
    "    def on_readable():\n",
    "        f.set_result(sock.recv(4096))\n",
    "\n",
    "    selector.register(sock.fileno(), EVENT_READ, on_readable)\n",
    "    chunk = yield from f  # Read one chunk.\n",
    "    selector.unregister(sock.fileno())\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyFuture:\n",
    "    def __init__(self):\n",
    "        self.result = None\n",
    "        self._callbacks = []\n",
    "\n",
    "    def add_done_callback(self, fn):\n",
    "        self._callbacks.append(fn)\n",
    "\n",
    "    def set_result(self, result):\n",
    "        self.result = result\n",
    "        print(\"cblist\", self._callbacks)\n",
    "        for fn in self._callbacks:\n",
    "            fn(self)\n",
    "            \n",
    "    def __iter__(self):\n",
    "        yield self\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Fetcher:\n",
    "    \n",
    "    def __init__(self, url, host):\n",
    "        self.url = url\n",
    "        self.host = host\n",
    "        self.response = b''  # Empty array of bytes.\n",
    "\n",
    "        \n",
    "    def fetch(self):\n",
    "        global stopped\n",
    "        sock = socket.socket()\n",
    "\n",
    "        sock.setblocking(False)\n",
    "        try:\n",
    "            sock.connect((self.host, 80))\n",
    "        except BlockingIOError:\n",
    "            pass\n",
    "\n",
    "        f = MyFuture()\n",
    "\n",
    "        def on_connected():\n",
    "            print('on connected cb ran')\n",
    "            f.set_result(None)\n",
    "        \n",
    "        \n",
    "        \n",
    "        selector.register(sock.fileno(),\n",
    "                          EVENT_WRITE,\n",
    "                          on_connected)\n",
    "        print(\"about to yield connection future\")\n",
    "        yield from f\n",
    "        print('connected!')\n",
    "        selector.unregister(sock.fileno())\n",
    "        request = 'GET {} HTTP/1.0\\r\\nHost: xkcd.com\\r\\n\\r\\n'.format(self.url)\n",
    "        sock.send(request.encode('ascii'))\n",
    "        yield from read_all(sock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetcher = Fetcher('/353/','xkcd.com')\n",
    "Task(fetcher.fetch())\n",
    "stopped = False\n",
    "loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "import urllib\n",
    "urls_todo = set() \n",
    "seen_urls = set()\n",
    "\n",
    "selector = DefaultSelector()\n",
    "stopped = False\n",
    "\n",
    "\n",
    "class Fetcher:\n",
    "    def __init__(self, host, url, level=0):\n",
    "        self.response = b''  # Empty array of bytes.\n",
    "        self.host = host\n",
    "        self.url = url\n",
    "        self.sock = None\n",
    "        self.level = level\n",
    "    # Method on Fetcher class.\n",
    "    def fetch(self):\n",
    "        self.sock = socket.socket()\n",
    "        self.sock.setblocking(False)\n",
    "        try:\n",
    "            #print(self.host)\n",
    "            self.sock.connect((self.host, 80))\n",
    "        except BlockingIOError:\n",
    "            #print(self.host,'========')\n",
    "            pass\n",
    "\n",
    "        # Register next callback.\n",
    "        selector.register(self.sock.fileno(),\n",
    "                          EVENT_WRITE,\n",
    "                          self.connected)\n",
    "    def parse_links(self):\n",
    "        links = set()\n",
    "        s = BeautifulSoup(self.response)\n",
    "        \n",
    "        for link in s.find_all('a',href = True):\n",
    "            #print('found some url',link['href'],'\\n')\n",
    "            #print(link)\n",
    "            url = urllib.parse.urlparse(link['href'])\n",
    "            #print(url)\n",
    "            #print(url)\n",
    "            #rint(self.host)\n",
    "            if (url.netloc)=='':\n",
    "                #print(url.netloc)\n",
    "                #self.host = url.netloc\n",
    "                if (url.path) == '':\n",
    "                    links.add((self.host,\"/\"))\n",
    "                else:\n",
    "                    links.add((self.host,url.path))\n",
    "            elif (url.netloc)!='':\n",
    "                if (url.path) == '':\n",
    "                    links.add((url.netloc,\"/\"))\n",
    "                else:\n",
    "                    links.add((url.netloc, url.path))\n",
    "                \n",
    "        return set(links)\n",
    "    \n",
    "\n",
    "    def connected(self, key, mask):\n",
    "        #print('connected!', flush=True)\n",
    "        selector.unregister(key.fd)\n",
    "        request = 'GET {} HTTP/1.0\\r\\nHost: {}\\r\\n\\r\\n'.format(self.url, self.host)\n",
    "        self.sock.send(request.encode('ascii'))\n",
    "\n",
    "        # Register the next callback.\n",
    "        selector.register(key.fd,\n",
    "                          EVENT_READ,\n",
    "                          self.read_response)\n",
    "\n",
    "    def read_response(self, key, mask):\n",
    "        global stopped\n",
    "        \n",
    "        chunk = self.sock.recv(4096)  # USUALLY 4k chunk size, here small\n",
    "        if chunk:\n",
    "            print(\"read chunk\", flush=True)\n",
    "            self.response += chunk\n",
    "        else:\n",
    "            selector.unregister(key.fd)\n",
    "            if self.level == 1:\n",
    "            #print(\"all read\", flush=True)\n",
    "                #selector.unregister(key.fd)  # Done reading.\n",
    "                links = self.parse_links() # return a set of links: (host,url) tuples\n",
    "                links = links.difference(seen_urls) \n",
    "                #print(links)\n",
    "                seen_urls.update(links) # // update the global links\n",
    "                #print(seen_urls)\n",
    "                #print(seen_urls)\n",
    "                #selector.unregister(key.fd)  # Done reading.\n",
    "                #stopped=True\n",
    "                for h in links:\n",
    "                    #print(h[0],h[1])\n",
    "                    #print(len(h),h)\n",
    "                    fetcher = Fetcher(h[0],h[1])\n",
    "                    #print(h[0])\n",
    "                    fetcher.fetch()\n",
    "                    urls_todo.add((h[0],h[1]))\n",
    "                #print(self.host,self.url)\n",
    "            urls_todo.remove((self.host,self.url))\n",
    "            #print(urls_todo)\n",
    "            if not urls_todo:\n",
    "                stopped = True\n",
    "                print('done')\n",
    "            #print(len(urls_todo))       \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "#stopped = False\n",
    "\n",
    "def loop():\n",
    "    while not stopped:\n",
    "        events = selector.select()\n",
    "        for event_key, event_mask in events:\n",
    "            callback = event_key.data\n",
    "            callback(event_key, event_mask)\n",
    "        #do fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = a[-1]\n",
    "del a[-1]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "read chunk\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhantang/anaconda/envs/py35/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "fetcher = Fetcher('xkcd.com', '/353/',1)\n",
    "fetcher.fetch()\n",
    "urls_todo.add((fetcher.host,fetcher.url))\n",
    "loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
